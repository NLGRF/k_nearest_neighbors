{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors Classification from Scratch with NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-nearest neighhbors (KNN) is a machine learning algorithm that can be used both for classification and regression purposes. It falls under the category of supervised learning algorithms that predicts target values for unseen observations. In other words, it operates on labelled datasets and predicts either a class (classification) or a numeric value (regression) for the test data.\n",
    "\n",
    "In this post, I will be implementing KNN classification algorithm from scratch using NumPy library only. As I have mentioned in my previous posts, I won't be using an already implemented version of knn algorithm with the help of sklearn, I will be implementing it with only NumPy and will be using other libraries for creating and using datasets and data visualization purposes.\n",
    "\n",
    "Considering there are many useful resources out there that explains the fundamentals of knn, I want to get into coding part immediately. Let's do some classification!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the libraries and functions that will be used throughout the implementation:\n",
    "* `numpy`: It is used for numerical computation of multidimensional arrays, obviously.\n",
    "* `make_classification`: We will use this to create our very own classification dataset. We can decide how many classes and features we want, whether we want samples to be clustered and etc.\n",
    "* `matplotlib`: It will be used when we want to visualize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidian_distance(a, b):\n",
    "    return np.sqrt(np.sum((a-b)**2, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In knn algorithm, we will need a function to calculate the distances between training data points and the data that we would like to classify. Here, I've chosen the euclidian distance as it is the widely used type in machine learning applications. One can try classifying with other distance metrics such as Manhattan distance, Chebychev  distance and etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kneighbors(X_test, return_distance=False):\n",
    "       \n",
    "        dist = []\n",
    "        neigh_ind = []\n",
    "        \n",
    "        point_dist = [euclidian_distance(x_test, X_train) for x_test in X_test]\n",
    "\n",
    "        for row in point_dist:\n",
    "            enum_neigh = enumerate(row)\n",
    "            sorted_neigh = sorted(enum_neigh, key=lambda x: x[1])[:n_neighbors]\n",
    "    \n",
    "            ind_list = [tup[0] for tup in sorted_neigh]\n",
    "            dist_list = [tup[1] for tup in sorted_neigh]\n",
    "    \n",
    "            dist.append(dist_list)\n",
    "            neigh_ind.append(ind_list)\n",
    "        \n",
    "        if return_distance:\n",
    "            return np.array(dist), np.array(neigh_ind)\n",
    "        \n",
    "        return np.array(neigh_ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practical aspect of knn, we try to find the neighbors, the closest data points to the data that we want to classify. Whether a data point is close or not is determined by our euclidian distance function implemented above. Here, the actual value of the distance between data points does not matter, rather, we are interested in the order of those distances. We pick a number as our hyperparameter (k) before the training process and pick k-neareset neighbors to our data points we want to classify during training. Hence, for instance when we say 5-nearest neighbors, we mean the first 5 closest data points. \n",
    "\n",
    "In the `kneighbors` function above, we find the distances between each point in test dataset (the data points we want to classify) and the rest of the dataset, that is the training data. We store those distances in `point_dist` in which each row corresponds to a list of distances between one test data point and all of the training data. Hence, we go over each row, enumerate it and then sort it according to the distances. The reason we enumerate each row is because we don't want to lose the indices of training data points that we calculated the distances with, since we are going to refer them later. \n",
    "\n",
    "Consequently, `sorted_neigh` holds the first k-nearest neighbors of our test data points and they are sorted according to their euclidian distances. We, then, extract indices and distance values from `sorted_neigh` and return them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test, weights='uniform'):\n",
    "        \n",
    "        class_num = 3\n",
    "        \n",
    "        if weights=='uniform':\n",
    "            neighbors = kneighbors(X_test)\n",
    "            y_pred = np.array([np.argmax(np.bincount(y_train[neighbor])) for neighbor in neighbors])\n",
    "        \n",
    "            return y_pred \n",
    "    \n",
    "        if weights=='distance':\n",
    "        \n",
    "            dist, neigh_ind = kneighbors(X_test, return_distance=True)\n",
    "        \n",
    "            inv_dist = 1/dist\n",
    "            \n",
    "            mean_inv_dist = inv_dist / np.sum(inv_dist, axis=1)[:, np.newaxis]\n",
    "            \n",
    "            proba = []\n",
    "            \n",
    "            for i, row in enumerate(mean_inv_dist):\n",
    "                \n",
    "                row_pred = self.y_train[neigh_ind[i]]\n",
    "                \n",
    "                for k in range(class_num):\n",
    "                    indices = np.where(row_pred==k)\n",
    "                    prob_ind = np.sum(row[indices])\n",
    "                    proba.append(np.array(prob_ind))\n",
    "        \n",
    "            predict_proba = np.array(proba).reshape(X_test.shape[0], class_num)\n",
    "            \n",
    "            y_pred = np.array([np.argmax(item) for item in predict_proba])\n",
    "            \n",
    "            return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finding the k-nearest neighbors, we try to predict the classes that our test data points belong to. Here, we have k neighbors and each neighbor has a vote in deciding the class label. However, voting mechanism may vary according to the chosen criterion. Here, in the `predict` function above, if `weights` are chosen as `uniform` it means that each neighbor has the equal vote (weight) in deciding the class label irrespective of their distances. \n",
    "\n",
    "Let's say we have 5-nearest neighbors of our test data point, 3 of them belong to class A and 2 of them belong to class B. We disregard the distances of neighbors and conclude that the test data point belongs to the class A since the majority of neighbors are a part of class A. However, if `weights` are chosen as `distance`, then this means the distances of neighbors do matter, indeed. Hence, whichever neighbor that is closest to the test data point has more weight (vote) proportional to the inverse of their distances. So regarding the aforementioned example, if those 2 points belonging the class A are a lot closer to the test data point than the other 3 points, then, that may play a bigger role in deciding the class label for data point.\n",
    "\n",
    "In `predict` function, it is quite easy to predict the label for data points if `weights` are `uniform`. First, we get the indices of neighbors and then use those indices to get their corresponding class labels from training dataset. Each row in `neighbors` corresponds to the set of neighbors that each test data point has. We then find the occurences of class labels using `numpy`'s `bincount` function and get the index of the maximum occurence which corresponds to the predicted class label. \n",
    "\n",
    "Things get a little messier if we have `weights` chosen as `distance`. In this case, we find the mean inverse of neighbor distances and calculate class probabilities for each test data point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(X_test, y_test):\n",
    "        y_pred = predict(X_test)\n",
    "        \n",
    "        return float(sum(y_pred == y_test))/ float(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have implemented the `score` function as a very simple accuracy metric used in classification problems widely. We simply return the percentage of correctly classified labels. It is pretty straightforward! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples = 1000, n_features=2, n_redundant=0, n_informative=2,\n",
    "                             n_clusters_per_class=1, n_classes=3, random_state=21)\n",
    "\n",
    "\n",
    "mu = np.mean(X, 0)\n",
    "sigma = np.std(X, 0)\n",
    "\n",
    "X = (X - mu ) / sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is time to create the dataset that we will be testing our knn algorithm upon. We make use of `sklearn.dataset`'s `make_classification` function to populate the dataset. Afterwards, we normalize the each data point by subtracking the mean and then dividing by the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 5\n",
    "\n",
    "data = np.hstack((X, y[:, np.newaxis]))\n",
    "        \n",
    "np.random.shuffle(data)\n",
    "\n",
    "split_rate = 0.7\n",
    "\n",
    "train, test = np.split(data, [int(split_rate*(data.shape[0]))])\n",
    "\n",
    "X_train = train[:,:-1]\n",
    "y_train = train[:, -1]\n",
    "\n",
    "X_test = test[:,:-1]\n",
    "y_test = test[:, -1]\n",
    "\n",
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the dataset, we simply implement a random split for our training phase, hence obtain our training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[698, 488, 341, 122, 306],\n",
       "       [317, 539, 621, 321, 206],\n",
       "       [  8, 322,  66,  63, 626],\n",
       "       ...,\n",
       "       [680, 584, 270, 236, 686],\n",
       "       [ 57, 357, 277, 448, 628],\n",
       "       [  2, 382, 566, 269,  55]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kneighbors(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to test our code of knn implementation. We get the k-nearest neighbors of our test data set. Do notice that, each row is related to each data point in our test set and elements in each row correspond to the indices of neighbors of the test data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 1, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 0, 2, 0, 1, 1, 0, 2, 1, 2, 1, 0, 0, 1, 0, 0, 1, 2, 0,\n",
       "       2, 2, 0, 0, 2, 0, 2, 1, 2, 0, 0, 2, 0, 1, 0, 0, 2, 0, 2, 2, 0, 2,\n",
       "       1, 1, 2, 0, 2, 1, 0, 2, 0, 2, 0, 2, 1, 2, 1, 0, 0, 1, 1, 2, 1, 1,\n",
       "       2, 1, 2, 1, 0, 1, 1, 1, 2, 1, 2, 0, 0, 2, 2, 2, 0, 2, 0, 1, 1, 0,\n",
       "       2, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 0, 2, 0, 1, 0, 2, 1, 1, 2, 1,\n",
       "       2, 2, 0, 1, 1, 2, 2, 0, 2, 2, 1, 0, 2, 1, 1, 2, 1, 1, 0, 2, 0, 1,\n",
       "       1, 2, 1, 2, 1, 2, 0, 1, 2, 1, 2, 0, 1, 2, 2, 1, 1, 1, 0, 1, 2, 0,\n",
       "       1, 0, 1, 0, 1, 1, 1, 0, 2, 1, 1, 2, 1, 2, 0, 2, 2, 2, 2, 1, 0, 1,\n",
       "       2, 2, 1, 1, 1, 0, 0, 0, 2, 1, 2, 0, 0, 1, 1, 1, 2, 1, 0, 1, 1, 0,\n",
       "       2, 0, 0, 0, 1, 1, 1, 0, 2, 2, 0, 1, 0, 0, 1, 1, 1, 1, 2, 1, 1, 0,\n",
       "       1, 2, 0, 1, 2, 1, 1, 2, 0, 1, 0, 2, 0, 1, 2, 1, 2, 2, 0, 0, 1, 0,\n",
       "       0, 1, 2, 1, 1, 0, 1, 2, 2, 0, 0, 2, 1, 2, 1, 1, 0, 2, 1, 1, 2, 1,\n",
       "       1, 1, 2, 2, 0, 2, 1, 0, 2, 1, 0, 1, 0, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, `predict` function outputs the predicted class labels of the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9733333333333334"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out, our implementation did a really good job given the high accuracy score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Implementation of KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class KNearestNeighbors():\n",
    "    def __init__(self, X_train, y_train, n_neighbors=5, weights = 'uniform'):\n",
    "    \n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.weights = weights\n",
    "        \n",
    "        self.class_num = 3\n",
    "\n",
    "        \n",
    "    def euclidian_distance(self, a, b):     \n",
    "        return np.sqrt(np.sum((a-b)**2, axis=1))\n",
    "    \n",
    "    \n",
    "    \n",
    "    def kneighbors(self, X_test, return_distance=False):\n",
    "       \n",
    "        dist = []\n",
    "        neigh_ind = []\n",
    "        \n",
    "        point_dist = [self.euclidian_distance(x_test, self.X_train) for x_test in X_test]\n",
    "\n",
    "        for row in point_dist:\n",
    "            enum_neigh = enumerate(row)\n",
    "            sorted_neigh = sorted(enum_neigh, key=lambda x: x[1])[:self.n_neighbors]\n",
    "    \n",
    "            ind_list = [tup[0] for tup in sorted_neigh]\n",
    "            dist_list = [tup[1] for tup in sorted_neigh]\n",
    "    \n",
    "            dist.append(dist_list)\n",
    "            neigh_ind.append(ind_list)\n",
    "        \n",
    "        if return_distance:\n",
    "            return np.array(dist), np.array(neigh_ind)\n",
    "        \n",
    "        return np.array(neigh_ind)\n",
    "         \n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        \n",
    "        if self.weights=='uniform':\n",
    "            neighbors = self.kneighbors(X_test)\n",
    "            y_pred = np.array([np.argmax(np.bincount(self.y_train[neighbor])) for neighbor in neighbors])\n",
    "        \n",
    "            return y_pred \n",
    "    \n",
    "        if self.weights=='distance':\n",
    "        \n",
    "            dist, neigh_ind = self.kneighbors(X_test, return_distance=True)\n",
    "        \n",
    "            inv_dist = 1/dist\n",
    "            \n",
    "            mean_inv_dist = inv_dist / np.sum(inv_dist, axis=1)[:, np.newaxis]\n",
    "            \n",
    "            proba = []\n",
    "            \n",
    "            for i, row in enumerate(mean_inv_dist):\n",
    "                \n",
    "                row_pred = self.y_train[neigh_ind[i]]\n",
    "                \n",
    "                for k in range(self.class_num):\n",
    "                    indices = np.where(row_pred==k)\n",
    "                    prob_ind = np.sum(row[indices])\n",
    "                    proba.append(np.array(prob_ind))\n",
    "        \n",
    "            predict_proba = np.array(proba).reshape(X_test.shape[0], self.class_num)\n",
    "            \n",
    "            y_pred = np.array([np.argmax(item) for item in predict_proba])\n",
    "            \n",
    "            return y_pred\n",
    "            \n",
    "    def score(self, X_test, y_test):\n",
    "        y_pred = self.predict(X_test)\n",
    "        \n",
    "        return float(sum(y_pred == y_test))/ float(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing all necessary functions, it is pretty easy to create the class implementation of knn. Here only newly added function is the `__init__` function for our class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Our Implementation with Sklearn’s KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
